python3 -m pip install --user kaggle
mkdir /home/hadoop/.kaggle
aws s3 cp s3://project-mahesh/kaggle.json .
mv kaggle.json /home/hadoop/.kaggle
~/.local/bin/kaggle datasets download -d tanay001/nseindia-futures-options-daily --path /home/hadoop/ --unzip
hdfs dfs -put fobhav.csv  /user/hadoop/
rm fobhav.csv
spark-submit --master yarn --deploy-mode cluster --conf spark.sql.catalogImplementation=hive s3://project-mahesh/steps/step1.py
aws s3 cp s3://project-mahesh/steps/mysql-connector-java-8.0.23.jar .
sudo cp mysql-connector-java-8.0.23.jar /usr/lib/sqoop/lib/
sqoop export --connect jdbc:mysql://db.cxeefdcxn7ed.us-east-1.rds.amazonaws.com/db  --driver com.mysql.jdbc.Driver --username admin --password 123456789 --table nse20 --hcatalog-database default --hcatalog-table nse20
sqoop import --connect jdbc:mysql://db.cxeefdcxn7ed.us-east-1.rds.amazonaws.com/db --username admin --password 123456789 --query "select * from nse20 where \$CONDITIONS" --m 1 --target-dir /user/root/sqoop1
spark-submit --master yarn --deploy-mode cluster --conf spark.sql.catalogImplementation=hive s3://project-mahesh/steps/step3.py
sqoop export --connect jdbc:mysql://db.cxeefdcxn7ed.us-east-1.rds.amazonaws.com/db --driver com.mysql.jdbc.Driver --username admin --password 123456789 --table nse20 --hcatalog-database default --hcatalog-table nse10
sqoop import --connect jdbc:mysql://db.cxeefdcxn7ed.us-east-1.rds.amazonaws.com/db --username admin --password 123456789 --incremental append --query "select * from nse20 where \$CONDITIONS" --m 1 --target-dir /user/root/sqoop2  --check-column id --last-value 16
spark-submit --master yarn --deploy-mode cluster --conf spark.sql.catalogImplementation=hive s3://project-mahesh/steps/step5.py
spark-submit --master yarn --deploy-mode cluster --conf spark.sql.catalogImplementation=hive s3://project-mahesh/steps/EDA.py